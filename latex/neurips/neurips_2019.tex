\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{A Common Graph Representation for\\Source Code and Developer Documentation}

\author{Breandan Considine\\
breandan.considine@mail.mcgill.ca\\
McGill University}

\begin{document}

\maketitle

\begin{abstract}
Semantic information plays a key role in the code search and synthesis settings. In this work, we propose a graph-based representation for source code and natural language which incorporates semantic and relational features from both domains. We apply this graph to a parsing a corpus of code and developer documents, and demonstrate the effectiveness of a common graph-based representation on three downstream tasks: code search, document recommendation and link prediction.
\end{abstract}

\section{Background and motivation}

In addition to its syntactic structure, source code contains a rich denotational and operational semantics~\citep{henkel2018code}. To effectively reason about code in semantically similar but syntactically diverse settings requires models which incorporate features from the call graph~\citep{gu2016deep, 10.1145/3361242.3362774} and surrounding typing context~\citep{allamanis2017learning}. Many semantic features, such as data and control flow~\citep{si2018learning} can be represented as a directed acyclic graph (DAG), which admits linear-time solutions to a number of graph problems, including topological sorting, single-source shortest path and reachability queries.

DAGs also have important applications in natural language parsing~\citep{sagae2008shift, quernheim2012dagger}. Various attempts to build semantic representations for natural language have been proposed, notably the pointer network architecture~\citep{vinyals2015pointer, vinyals2015order}. Pointer networks help to capture permutation-invariant semantic relations between natural language entities, and have important applications in dependency parsing~\citep{ma2018stack}, named-entity recognition~\citep{lample2016neural}, and other tasks where sequence representations fall short. \citet{li2017code} extend pointer networks with a copy-mechanism to handle out-of-vocabulary code tokens.

Content recommendation doc-to-doc (D2D) and code-to-code (C2C) is a straightforward application of link prediction~\citep{zhang2018link} and code embedding~\citep{gu2018deep} techniques, but cross-domain transfer largely remains unsolved. \citet{robillard2015recommending} first explore the task of predicting reference API documentation from source code using manual annotation. Prior work also studies the association between comments and code entities~\citep{panthaplackel2020associating} using machine learning, but only within source code.

Maintainers of widely-used software projects often publish web-based documentation, typically stored in markup languages like HTML or Markdown. These files contain a collection of natural language sentences, markup, and hyperlinks to other documents. Both the link graph and the document AST contain important semantic information: the markup describes the text in relation to the other entities in the document hierarchy~\citep{yang2016hierarchical}, while the link graph describes the relationship between the parent document and related documents or source code entities. Documents occasionally contains hyperlinks to source code, but source code rarely contains links to developer documents.

Some programming languages allow users to specify which type of values will inhabit a given variable at runtime. Types allow the compiler to reason about certain properties like nullity~\citep{ekman2007pluggable} and shape~\citep{considine2019kotlingrad}. While types many not appear explicitly in source code, they can often be inferred from the surrounding context using a dataflow graph (DFG). The Java language recently introduced local variable type inference~\citet{liddell2019analyzing}, which allows variable types to be omitted, and later inferred by the compiler.

\section{Proposed approach}

Given a single token in either source code or developer documentation and its semantic context, what are the most relevant source code or documentation entities? We would like to infer which documents are relevant to a particular token, based on the documentation or code context. To infer links between these two domains requires building a multi-relational graph, using features extracted from both natural language and code. Following \citet{si2018learning, gu2018deep, 10.1145/3361242.3362774}, we use the dataflow graph and type environment as features for code, and following \citet{yang2016hierarchical, zhang2018link}, use the markdown hierarchy and link graph as the documentation graph.

Due to the sparsity of hyperlinks between code and documentation, we need a heuristic to relate the document graph to source code entities. We observe that in practice rare tokens which co-occur in unrelated documents often refer to a common entity, a heuristic which developers often exploit when searching for errors and code tokens. Co-occurrence of a salient lexical token in both the document and source code is a strong indicator the two are related, even though they may not be linked. If we can recover this relationship without seeing the lexical token itself, but only using dataflow and type-related information, this indicates our representation is learning a useful feature.

\section{Data availability and computational requirements}

Java, one of the most popular programming languages on GitHub, is a statically typed language with a large amount of API documentation on the web. It has a variety of tools for parsing and analyzing both code~\citep{kovalenko2019pathminer} and natural language~\citep{manning2014stanford, grella2018non}, and an extensive set of documentation. Our dataset consists of Java repositories on GitHub, and their accompanying docs collected from the Zeal software documentation aggregator. All projects in our dataset have a collection of source code files and multiple related repositories on GitHub.

We construct two datasets consisting of naturally occurring links between developer documentation and source code, and a surrogate set of links constructed by lexical tokens available in both projects. Our target is recovery of ground truth links in the test set and surrogate links in the lexical matching graph. By identifying tokens using, e.g. a pointer network architecture, and adding weighted edges between source code and documentation, we evaluate which tokens refer code-like fragments and other entities which refer to the selected token. We evaluate our approach on D2D and C2C link retrieval, as well as precision and recall on the surrogate link graph.

To perform our experiments, require a large number of CPUs for semantic parsing, link extraction and graph preprocessing, and a single P100 GPU for training a graph neural network. We have applied and received access to the Niagra CC cluster.

\bibliography{neurips_2019}
\bibliographystyle{plainnat}
\end{document}