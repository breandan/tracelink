\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphviz}
\usepackage{auto-pst-pdf}
\usepackage{listings}
\usepackage{float}
\usepackage{xcolor}
\usepackage{framed}
% Code listings
%\usepackage[scaled]{beramono}
\usepackage{fontspec}
\setmonofont{JetBrains Mono}
\usepackage{ocr}
\usepackage[skins,breakable,listings]{tcolorbox}

\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows}

\tikzset{
    ->,  % makes the edges directed
    >=stealth’, % makes the arrow heads bold
    node distance=3cm, % specifies the minimum distance between two nodes. Change if necessary.
    every state/.style={thick, fill=gray!10}, % sets the properties for each ’state’ node
    initial text=$ $, % sets the text that appears on the start arrow
}

\lstdefinelanguage{kotlin}{
    comment=[l]{//},
    commentstyle={\color{gray}\ttfamily},
    emph={delegate, filter, firstOrNull, forEach, it, lazy, mapNotNull, println, repeat, assert, with, head, tail, len, return@},
    numberstyle=\noncopyable,
    emphstyle={\color{OrangeRed}},
    identifierstyle=\color{black},
    keywords={abstract, actual, as, as?, break, by, class, companion, continue, data, do, dynamic, else, enum, expect, false, final, for, fun, get, if, import, in, infix, interface, internal, is, null, object, open, operator, override, package, private, public, return, sealed, set, super, suspend, this, throw, true, try, typealias, val, var, vararg, when, where, while, tailrec, reified},
    keywordstyle={\color{NavyBlue}\bfseries},
    morecomment=[s]{/*}{*/},
    morestring=[b]",
    morestring=[s]{"""*}{*"""},
    ndkeywords={@Deprecated, @JvmField, @JvmName, @JvmOverloads, @JvmStatic, @JvmSynthetic, Array, Byte, Double, Float, Boolean, Int, Integer, Iterable, Long, Runnable, Short, String},
    ndkeywordstyle={\color{BurntOrange}\bfseries},
    sensitive=true,
    stringstyle={\color{ForestGreen}\ttfamily},
    literate={`}{{\char0}}1
}

\usepackage{lipsum}
\tcbuselibrary{listings,breakable}

\newtcblisting{javalisting}[1][]{%
    breakable=false,
    listing only,
    boxsep=-1.5pt,
    top=-1pt,
    bottom=-0.5pt,
    listing options={
        language=kotlin,
        basicstyle=\ttfamily\small,
        numberstyle=\footnotesize\noncopyable,
        tabsize=2,
        numbers=right,
        breaklines=true,
        inputencoding=utf8,
        escapeinside={(*@}{@*)},
        #1
    },
}

\tcbset{
    enhanced jigsaw,
    breakable,
    listing only,
    boxsep=-1pt,
    top=-1pt,
    bottom=-0.5pt,
    right=-0.5pt,
    overlay first={
        \node[black!50] (S) at (frame.south) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.south west) -- (S) -- (frame.south east);
    },
    overlay middle={
        \node[black!50] (S) at (frame.south) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.south west) -- (S) -- (frame.south east);
        \node[black!50] (S) at (frame.north) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.north west) -- (S) -- (frame.north east);
    },
    overlay last={
        \node[black!50] (S) at (frame.north) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.north west) -- (S) -- (frame.north east);
    },
    before={\par\vspace{10pt}},
    after={\par\vspace{\parskip}\noindent}
}

\newcommand*{\inlineimg}[1]{%
    \raisebox{-.3\baselineskip}{%
        \includegraphics[
            height=\baselineskip,
            width=\baselineskip,
            keepaspectratio,
        ]{#1}%
    }%
}

\definecolor{slightgray}{rgb}{0.90, 0.90, 0.90}

\usepackage{soul}
\makeatletter
\def\SOUL@hlpreamble{%
    \setul{}{3.0ex}%
    \let\SOUL@stcolor\SOUL@hlcolor%
    \SOUL@stpreamble%
}
\makeatother

\newcommand{\tinline}[1]{%
    \begingroup%
    \sethlcolor{slightgray}%
    \hl{\ttfamily\footnotesize #1}%
    \endgroup
}

\title{Learning to parse developer documentation}

\author{Breandan Considine\\
breandan.considine@mail.mcgill.ca\\
McGill University}

\begin{document}

\maketitle

\begin{abstract}
Semantic information plays a key role in the code search and synthesis settings. In this work, we propose a regular expression synthesizer for link prediction in source code and API documentation which incorporates semantic and relational information the surrounding context. We apply our synthesizer to a link prediction task on a corpus of Java code and developer docs, and demonstrate the effectiveness of a model-based representation learning for link prediction in the source-to-source and doc-to-doc setting.
\end{abstract}

\section{Introduction}

In addition to its syntax, source code contains a rich denotational and operational semantics~\citep{henkel2018code}. To effectively reason about code in semantically similar but syntactically diverse settings requires models which incorporate features from the call graph~\citep{gu2016deep, liu2019neural} and surrounding typing context~\citep{allamanis2017learning}. Many semantic features, such as data and control flow~\citep{si2018learning} can be represented by a directed acyclic graph (DAG), which admits linear-time solutions to many graph problems, including topological sorting, single-source shortest path and reachability queries.

Natural language has also developed a rich set of graph-based representations, including \citet{reddy2016transforming}'s and other typed attribute grammars which can be used to reason about syntactic and semantic relations between natural language entities. The pointer network architecture~\citep{vinyals2015pointer, vinyals2015order} can be used to construct permutation-invariant semantic relations between entities, and has important applications in dependency parsing~\citep{ma2018stack}, named-entity recognition~\citep{lample2016neural}, and other semantic parsing tasks where sequence-based representations fall short. \citet{li2017code} extend pointer networks with a copy-mechanism to handle out-of-vocabulary code tokens.

Existing work has studied doc-to-doc (D2D) and code-to-code (C2C) entity linking by applying link prediction~\citep{zhang2018link} and code embedding~\citep{gu2018deep} techniques, but cross-domain transfer remains challenging. \citet{robillard2015recommending, robillard2017demand} first explore the task of predicting reference API docs from source code using manual annotation. Prior work also studies the association between comments and code entities~\citep{iyer2018mapping, panthaplackel2020associating} using machine learning, but only within source code.

Maintainers of widely-used software projects often publish web-based developer docs, typically stored in markup languages like HTML or Markdown. These files contain a collection of natural language sentences, markup, and hyperlinks to other documents and source code entities. Both the document tree and link graph contain important semantic information: the markup describes the text in relation to the other entities in the document hierarchy~\citep{yang2016hierarchical}, while the link graph describes the relationship between the parent document and related documents or source code entities. Documents occasionally contain hyperlinks to source code, but source code rarely contains links to developer documents.

Prior work has studied dataflow and type-based representations~\citep{si2018learning, gu2018deep, liu2019neural}, as well as document hierarchy~\citep{yang2016hierarchical} and link-based~\citep{zhang2018link} information. In our work, we attempt to bridge these domains by learning a common representation for both programming and natural languages.

Unlike natural language where polysemy is common, named entities in most programming languages are relatively unique, even across unrelated APIs. Given a single token in source code or documentation and its surrounding context, a skilled developer is quickly able to locate the entity, even without prior familiarity with the API in question. For example, we observe the string \tinline{AbstractSingletonProxyFactoryBean} has the following properties:

\begin{enumerate}
    \item The string is camel-case, indicating it refers to an entity in a camel-case language.
    \item The string contains the substring \tinline{Bean}, a common token in the Java language.
    \item The string begins with a capital letter, indicating it refers to a class or interface.
\end{enumerate}

Developers often use a tool called \tinline{grep} to locate files, which accepts queries written in a domain specific language (DSL) known as regular expression (regex). Skilled \tinline{grep} users are able to rapidly construct a query which retrieves the document with high probability whilst omitting irrelevant results. Assuming the aforementioned entity in question exists on a filesystem, one might simply execute the following command to locate it:

\centerline{\tinline{\$ grep -r --include .java "class AbstractSingletonProxyFactoryBean" .}}

%Alternatively, the following search produces the correct entity as the first result: \small{\url{https://www.google.com/search?q=github+"class+AbstractSingletonProxyFactoryBean"}}.

We hypothesize it is possible to construct a short query which uniquely identifies any named entity (assuming it exists) in a corpus of software documents and furthermore, it is possible to learn a program which synthesizes a query retrieving the canonical entity with high probability, given a named entity reference and its surrounding documentation context.

\section{Background}


\newcommand{\mor}{\ensuremath{\;\mid\;}}
\newcommand{\code}[1]{\ensuremath{\texttt{\tinline{#1}}}}
\newcommand{\bnfrl}[1]{\ensuremath{\langle#1\rangle}}

Let $\Sigma = \{$\tinline{A}, \tinline{a}, \ldots, \tinline{Z}, \tinline{z}, \tinline{0}, \tinline{1}, \ldots, \tinline{9}, \tinline{\$}, \tinline{\textasciicircum}$\}$. Our language $J_{<}$ has the following productions:

\begin{equation}
    \bnfrl{exp} ::= \bnfrl{exp}\bnfrl{exp} \mor \bnfrl{exp}|\bnfrl{exp} \mor \code{(}\bnfrl{exp}\code{)} \mor \alpha \in \Sigma \mor \bnfrl{exp}\code{*} \mor
    \code{.}
\end{equation}

An expression in $J_<$ is a regular expression, reducible to a non-deterministic finite automaton (NFA) using Glushkov's algorithm ~\citep{glushkov1961abstract}, as shown in \autoref{fig:regex_to_nfa}. NFA are reducible to both deterministic finite automata (DFA) using the powerset construction~\citep{rabin1959finite} and regular expressions using Arden's Lemma~\citep{arden1961delayed}.

\begin{figure}
    \begin{tikzpicture}
        \node[state, initial, accepting] (q1) {$q_1$};
        \node[state, left of=q1] (q2) {$q_2$};
        \node[state, left of=q2] (q3) {$q_3$};
        \node[state, right of=q1] (q4) {$q_4$};
        \node[state, above right of=q4] (q5) {$q_5$};
        \node[state, below right of=q4] (q6) {$q_6$};
        \draw
%        (q1) edge[loop above] (q1)
        (q1) edge[above] node{a} (q2)
        (q1) edge[above] node{b} (q4)
        (q2) edge[bend right] node{a} (q3)
        (q3) edge[bend right] node{b} (q2)
        (q4) edge[loop above] node{a} (q4)
        (q4) edge[above] node{a} (q5)
        (q6) edge[above] node{a} (q4)
        (q5) edge[bend left] node{b} (q6)
        (q6) edge[bend left] node{a} (q5)
        ;
%        (q2) edge[bend left, above] node{0} (q3)
        ;
%        (q2) edge[bend left, above] node{0} (q3)
%        (q3) edge[bend left, below] node{0, 1} (q2);
    \end{tikzpicture}
    \caption{NFA corresponding to the regular expression \tinline{(a(ab)*)*(ba)*}.}
    \label{fig:regex_to_nfa}
\end{figure}

Formally, an NFA is a 5-tuple $\langle Q, \Sigma, \Delta, q_0, F \rangle$, where $Q$ is a finite set of states, $\Sigma$ is the alphabet, $\Delta :Q\times (\Sigma \cup \{\epsilon \})\rightarrow P(Q)$ is the transition function, $q_0 \in Q$ is the initial state and $F \subseteq Q$ are the terminal states. An NFA can be represented as a directed graph whose adjacency matrix is defined by the transition function, with edge labels representing symbols from the alphabet and binary node labels indicating whether the node is a terminal or nonterminal state.

We pose the problem as a few-shot generative modeling task, where the input is a local graph consisting of the query text and graph context, and the output is an adjacency matrix defining the NFA. Our goal is to synthesize an NFA which accepts the target document and no other documents or as few others as possible from the entire corpus.

\section{Method}

Let $\mathcal D$ be a document graph, constructed by semantically parsing the document's contents, and neighboring documents from the link graph. Let $\mathcal T$ be a code token, corresponding to a node in a document graph $\mathcal D$.  We train a meta learner $\mathcal M$ on the following objective:

\begin{equation}
    \mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} \mathcal M_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})]]
\end{equation}

Following prior work in few-shot meta-learning, our model is trained using a meta-training and test set, where the synthesizer constructs a regex to retrieve the documents, attends over the results and incrementally refines the query, then selects a single document to link. Our synthesizer is trained on a subgraph representing the local context, parsed from the document's contents and neighboring documents from the local link graph. We then compare precision and recall over the test set.

\section{Dataset}

Java, one of the most prolific programming languages on GitHub, is a statically typed language with a high volume of API documentation. Offering a variety of tools for source code~\citep{parr2013definitive, hosseini2013javaparser, kovalenko2019pathminer} and natural language~\citep{manning2014stanford, grella2018non} parsing, it is a convenient language for both analysis and implementation. Our dataset consists of Java repositories on GitHub, and their accompanying developer documents. All projects in our dataset have a collection of source code files and multiple related repositories on GitHub.

We construct two datasets consisting of naturally-occurring links between developer docs and source code, and a surrogate set of links constructed by matching lexical tokens available in both domains. Our target is recovery of ground truth links in the test set and surrogate links in the lexical matching graph. We first add weighted edges between code and docs, then evaluate our approach by predicting synthetic links between tokens contained in code fragments and markup entities which refer to the selected token. In addition, we evaluate our approach on both D2D and C2C link retrieval, as well as precision and recall on the surrogate link relations.

Our data consists of two complementary datasets: abstract syntax trees collected from Java source code and developer documentation. We use the astminer~\citep{kovalenko2019pathminer} library to parse Java code, jsoup~\citep{hedley2009jsoup} to parse HTML and Stanford's CoreNLP~\citep{manning2014stanford} library to parse dependency graphs from developer docs. Consider the following AST, parsed from the \href{https://www.eclipse.org/collections/}{Eclipse Collections} Java project:

\vspace{20pt}

\begin{figure}[H]
    \centering
    \input{ast}
%    \caption{This AST was parsed from the following code snippet:}
    \label{fig:ast}
\end{figure}

\vspace{-30pt}The AST depicted above was generated by parsing the following code snippet:

\begin{javalisting}
public void lastKey_throws() {
    new ImmutableTreeMap<>(new TreeSortedMap<>()).lastKey();
}
\end{javalisting}

Now consider the following dependency graph, taken from a Javadoc in the same project:

\begin{figure}[H]
    \centering
    \input{eng}
    \caption{This graph was parsed from the following comment: ``The advantages to using this container over a \footnotesize\tinline{Map<K, Collection<V>{}>} is that all of the handling of the value collection can be done automatically. It also allows implementations to further specialize in how duplicate values will be handled. Value collections with list semantics would allow duplicate values for a key, while those implementing set semantics would not. The value collections can never be empty.''}
    \label{fig:eng}
\end{figure}

Our goal is to connect these two graphs using a common model for source code and natural language. Absent any explicit \tinline{@link} or \tinline{@see} annotations, in order to relate these two graphs, we must somehow infer the shared semantic entities, which we can do for a subset using a simple lexical matching procedure.



%We then train four link prediction models on the following datasets:
%
%\begin{itemize}
%    \item Code graph (CG)
%    \item Documentation graph (DG)
%    \item Both code and documentation graphs separately (CDG)
%    \item Single graph containing code and documentation entities related by lexical matching (LEX)
%\end{itemize}
%
%For each model, we evaluate our trained model on link prediction in the same setting and every other setting.

\clearpage
\newpage

\bibliography{neurips_2019}
\bibliographystyle{plainnat}
\end{document}
