\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amssymb}       % blackboard math symbols
\usepackage{amsmath}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphviz}
\usepackage{auto-pst-pdf}
\usepackage{listings}
\usepackage{float}
\usepackage{xcolor}
\usepackage{framed}
% Code listings
%\usepackage[scaled]{beramono}
\usepackage{fontspec}
\setmonofont{JetBrains Mono}
\usepackage{ocr}
\usepackage[skins,breakable,listings]{tcolorbox}

\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{automata, positioning, arrows}

\tikzset{
    node distance=3cm, % specifies the minimum distance between two nodes. Change if necessary.
%    every state/.style={thick, fill=gray!10}, % sets the properties for each ’state’ node
    initial text=$ $, % sets the text that appears on the start arrow
}

\lstdefinelanguage{kotlin}{
    comment=[l]{//},
    commentstyle={\color{gray}\ttfamily},
    emph={delegate, filter, firstOrNull, forEach, it, lazy, mapNotNull, println, repeat, assert, with, head, tail, len, return@},
    numberstyle=\noncopyable,
    emphstyle={\color{OrangeRed}},
    identifierstyle=\color{black},
    keywords={abstract, actual, as, as?, break, by, class, companion, continue, data, do, dynamic, else, enum, expect, false, final, for, fun, get, if, import, in, infix, interface, internal, is, null, object, open, operator, override, package, private, public, return, sealed, set, super, suspend, this, throw, true, try, typealias, val, var, vararg, when, where, while, tailrec, reified},
    keywordstyle={\color{NavyBlue}\bfseries},
    morecomment=[s]{/*}{*/},
    morestring=[b]",
    morestring=[s]{"""*}{*"""},
    ndkeywords={@Deprecated, @JvmField, @JvmName, @JvmOverloads, @JvmStatic, @JvmSynthetic, Array, Byte, Double, Float, Boolean, Int, Integer, Iterable, Long, Runnable, Short, String},
    ndkeywordstyle={\color{BurntOrange}\bfseries},
    sensitive=true,
    stringstyle={\color{ForestGreen}\ttfamily},
    literate={`}{{\char0}}1
}

\usepackage{lipsum}
\tcbuselibrary{listings,breakable}

\newtcblisting{javalisting}[1][]{%
    breakable=false,
    listing only,
    boxsep=-1.5pt,
    top=-1pt,
    bottom=-0.5pt,
    listing options={
        language=kotlin,
        basicstyle=\ttfamily\small,
        numberstyle=\footnotesize\noncopyable,
        tabsize=2,
        numbers=right,
        breaklines=true,
        inputencoding=utf8,
        escapeinside={(*@}{@*)},
        #1
    },
}

\tcbset{
    enhanced jigsaw,
    breakable,
    listing only,
    boxsep=-1pt,
    top=-1pt,
    bottom=-0.5pt,
    right=-0.5pt,
    overlay first={
        \node[black!50] (S) at (frame.south) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.south west) -- (S) -- (frame.south east);
    },
    overlay middle={
        \node[black!50] (S) at (frame.south) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.south west) -- (S) -- (frame.south east);
        \node[black!50] (S) at (frame.north) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.north west) -- (S) -- (frame.north east);
    },
    overlay last={
        \node[black!50] (S) at (frame.north) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.north west) -- (S) -- (frame.north east);
    },
    before={\par\vspace{10pt}},
    after={\par\vspace{\parskip}\noindent}
}

\newcommand*{\inlineimg}[1]{%
    \raisebox{-.3\baselineskip}{%
        \includegraphics[
            height=\baselineskip,
            width=\baselineskip,
            keepaspectratio,
        ]{#1}%
    }%
}

\definecolor{slightgray}{rgb}{0.90, 0.90, 0.90}

\usepackage{soul}
\makeatletter
\def\SOUL@hlpreamble{%
    \setul{}{3.0ex}%
    \let\SOUL@stcolor\SOUL@hlcolor%
    \SOUL@stpreamble%
}
\makeatother

%\newcommand{\tinline}[1]{%
%    \begingroup%
%    \sethlcolor{slightgray}%
%    \hl{\ttfamily\footnotesize #1}%
%    \endgroup
%}
\newcommand*{\tinline}[1]{{\sethlcolor{slightgray}\ttfamily\footnotesize\relax\hl{#1}}}

\title{Learning to parse developer documentation}

\author{Breandan Considine\\
breandan.considine@mail.mcgill.ca\\
McGill University}

\begin{document}

\maketitle

\begin{abstract}
In this work, we propose a regular expression (regex) synthesizer for entity linking in source code and API documentation which incorporates semantic and relational features from the surrounding context. We demonstrate the effectiveness of our synthesizer on a link prediction task using a corpus of Java code and developer documentation, and demonstrate the effectiveness of model-based representation learning for interpretable link prediction in the source-to-source and doc-to-doc setting.
\end{abstract}

\section{Introduction}

Semantic information plays a key role in both natural and programming languages. In addition to its syntax, source code contains a rich denotational and operational semantics~\citep{henkel2018code}. To effectively reason about code in semantically similar but syntactically diverse settings requires models which incorporate features from the call graph~\citep{gu2016deep, gu2018deep, liu2019neural} and surrounding typing context~\citep{allamanis2017learning}. Many semantic features, such as data and control flow~\citep{si2018learning} can be represented using a directed acyclic graph (DAG), which admits linear-time solutions to a variety of graph problems, including topological sorting, single-source shortest path and reachability queries.

The field of natural language has also developed a rich set of graph-based representations, including \citet{reddy2016transforming}'s and other typed attribute grammars which can be used to reason about syntactic and semantic relations between natural language entities. In the pointer network architecture, \citet{vinyals2015pointer, vinyals2015order} emphasize the importance of constructing  permutation-invariant representations and show SOTA improvements in semantic labeling tasks from dependency parsing~\citep{ma2018stack}, named-entity recognition~\citep{lample2016neural}, and coreference resolution where sequence-based techniques often struggle. Pointer networks have been recently extended with a copy-mechanism~\citep{li2017code} to handle out-of-vocabulary code tokens.

These tools can be used for studying both source code~\citep{allamanis2017learning}  and documentation~\citep{yang2016hierarchical}. Entity alignment in doc-to-doc (D2D) and source-to-source (S2S) is a straightforward application of existing link prediction~\citep{zhang2018link} and code embedding~\citep{gu2018deep} techniques, but examples of cross-domain applications remain scarce. \citet{robillard2015recommending, robillard2017demand} first explore the task of suggesting reference API docs from source code using human feedback. Prior work also studies the relationship between comments and code entities~\citep{iyer2018mapping, panthaplackel2020associating} using machine learning, but only within source code.

Maintainers of popular software projects often publish web-based developer docs, typically in markup languages like HTML or Markdown~\citep{terrasa2018using}. These documents contain natural language sentences, markup, and hyperlinks to other documents and source code artifacts. Both the document and link graph contain important semantic information. The markup describes the text in relation to other entities in the document hierarchy~\citep{yang2016hierarchical}, while the link graph describes relationships between related documents or source code entities. To compensate for the sparsity of hyperlinks between code and documentation, new techniques are likely required.

Unlike natural languages where polysemy is a common phenomenon~\citep{ganea2016probabilistic}, most non-trivial tokens in source code are unique, even in large corpora. While the frequency of out-of-vocabulary (OOV) tokens presents a significant challenge for language modeling, it is an auspicious property for code search, where two lexical matches almost always refer to a single entity. Suppose we are given the string \tinline{AbstractSingletonFactoryBean}. We observe it has the following properties:

\begin{enumerate}
    \item The string is camel-case, indicating it refers to an entity in a camel-case language.
    \item The string contains the substring \tinline{Bean}, a common token in the Java language.
    \item The string begins with a capital letter, indicating it refers to a class or interface.
\end{enumerate}

Developers often use a tool called \tinline{grep} to locate files, which accepts queries written in the regular expression (regex) language, a domain specific language for string parsing and text search. Skilled \tinline{grep} users are able to rapidly construct a regex which retrieves the target entity with high probability whilst omitting irrelevant results. Assuming the entity exists on our filesystem, we can simply execute the following command to locate it:

\centerline{\tinline{\$ grep -rE --include *.java "(class|interface) AbstractSingletonFactoryBean" .}}

We hypothesize there exists a short regex which retrieves any named entity (assuming it exists) in a naturally occurring corpus of software artifacts. Given a named entity and its surrounding context, our goal is to synthesize a regex which selects only the link target, and as few other artifacts from the corpus as possible.

\section{Dataset}

Java is a statically-typed language with a high volume of API documentation. Offering a variety of tools for parsing source code~\citep{parr2013definitive, hosseini2013javaparser, kovalenko2019pathminer} and natural language~\citep{manning2014stanford, grella2018non}, it serves as a convenient language for both analysis and implementation. Our dataset consists of Java repositories on GitHub, and their accompanying documentation. All projects in our dataset have a collection of source code files and natural language documents.

We construct two datasets consisting of naturally-occurring links in developer docs, and a surrogate set of links constructed by matching lexical tokens in developer docs and source code. Our goal is recovery of ground truth links in the test set and surrogate links in the lexical matching graph. We evaluate our approach on both D2D and C2C link retrieval, as well as precision and recall on the surrogate link relations.

Our data consists of two complementary datasets: abstract syntax trees (ASTs) collected from Java source code and developer documentation. We use the astminer~\citep{kovalenko2019pathminer} library to parse Java code, jsoup~\citep{hedley2009jsoup} to parse HTML and Stanford's CoreNLP~\citep{manning2014stanford} library to parse dependency graphs from developer docs. Consider the following AST, parsed from API docmentation in the \href{https://www.eclipse.org/collections/}{Eclipse Collections} Java project:

\vspace{20pt}

\begin{figure}[H]
    \centering
    \input{ast}
%    \caption{This AST was parsed from the following code snippet:}
    \label{fig:ast}
\end{figure}

\vspace{-30pt}The AST depicted above was generated by parsing the following code snippet:

\begin{javalisting}
public void lastKey_throws() {
    new ImmutableTreeMap<>(new TreeSortedMap<>()).lastKey();
}
\end{javalisting}

Now consider the following dependency graph, taken from a Javadoc in the same project:

\begin{figure}[H]
``The advantages to using this container over a \footnotesize\tinline{Map<K, Collection<V>{}>} is that all of the handling of the value collection can be done automatically. It also allows implementations to further specialize in how duplicate values will be handled. Value collections with list semantics would allow duplicate values for a key, while those implementing set semantics would not.''\\\\

    \centering
    \input{eng}
    \caption{Dependency graph parsed from a JavaDoc comment, shown above.}
    \label{fig:eng}
\end{figure}

Our goal is to connect these two graphs using a common representation for source code and natural language. Absent any explicit \tinline{@link} or \tinline{@see} annotations, in order to relate these two graphs, we must somehow infer the shared semantic entities. We describe the problem in the following section.

\section{Problem}

Suppose we have a set of hyperlinks $H = \{h_0, \ldots, h_m\}$ and documents $D = \{d_0, \ldots d_n\}$. Each document contains a sequence of tokens $T = \{t_0, \ldots, t_p\}$. Each hyperlink $h_i$ is a tuple $\langle t_{anchor}, d_{source}, d_{target}\rangle$. We want a function $\mathcal G: (d_k, t_i; \theta) \mapsto \mathcal R$, parameterized by $\theta$, taking a token and its parent document, which produces a regular expression $\mathcal R: \mathcal D \rightarrow \mathcal D^{*}$ such that $L_{test}$ is minimized.

\section{Method}

\newcommand{\mor}{\ensuremath{\;|\;}}
\newcommand{\code}[1]{\ensuremath{\texttt{\tinline{#1}}}}
\newcommand{\bnfrl}[1]{\ensuremath{\langle#1\rangle}}

Let $\Sigma := $ \tinline{A} | \tinline{a} | \ldots | \tinline{Z} | \tinline{z} | \tinline{0} | \tinline{1} | \ldots | \tinline{9} | \tinline{ } | \tinline{\textasciicircum}. Our language $J_{<}$ has the following productions:

\begin{equation}\label{eq:grammar}
    \bnfrl{exp} := \bnfrl{exp}\cdot\bnfrl{exp} \mor \bnfrl{exp}\code{|}\bnfrl{exp} \mor \code{(}\bnfrl{exp}\code{)} \mor \Sigma \mor \bnfrl{exp}\code{*} \mor \code{!}\bnfrl{exp} \mor \code{.}
\end{equation}

Where ` $\cdot$ ' indicates concatenation. $J_<$ is a regular language, reducible to a non-deterministic finite automaton (NFA) using the \citet{glushkov1961abstract} algorithm as shown in \autoref{fig:regex_to_nfa} (independently discovered by McNaughton-Yamada-Thompson). NFA are reducible to both deterministic finite automata (DFA) using the powerset construction~\citep{rabin1959finite} and regular expressions using Arden's Lemma~\citep{arden1961delayed}. Regular expressions can also be converted directly to DFA as described by \citet{brzozowski1964derivatives} and \citet{berry1986regular}.

% https://www3.nd.edu/~kogge/courses/cse30151-fa17/Public/other/tikz_tutorial.pdf
% Glushkov's algorithm: https://www.irif.fr/~jep/PDF/MPRI/MPRI.pdf#subsection.3.5.2
\begin{figure}
    \begin{tikzpicture}[->, >=stealth,]
        \node[state, initial above, accepting] (q0) {$q_0$};
        \node[state, left of=q0] (q1) {$q_1$};
        \node[state, accepting, left of=q1] (q2) {$q_2$};
        \node[state, accepting, right of=q0] (q3) {$q_3$};
        \node[state, above right of=q3] (q4) {$q_4$};
        \node[state, accepting, below right of=q3] (q5) {$q_5$};
        \draw
%        (q0) edge[loop above] (q0)
        (q0) edge node{\tinline b} (q1)
        (q0) edge node{\tinline a} (q3)
        (q1) edge[bend right] node{\tinline a} (q2)
        (q2) edge[bend right] node{\tinline b} (q1)
        (q3) edge[loop above] node{\tinline a} (q3)
        (q3) edge node{\tinline a} (q4)
        (q5) edge node{\tinline a} (q3)
        (q4) edge[bend left] node{\tinline b} (q5)
        (q5) edge[bend left] node{\tinline a} (q4)
    \end{tikzpicture}
    \caption{NFA corresponding to the regex \tinline{(a(ab)*)*(ba)*}, where $q_{\{0, 2, 3, 5\}} \in F$.}
    \label{fig:regex_to_nfa}
\end{figure}

Formally, an NFA is a 5-tuple $\langle Q, \Sigma, \Delta, q_0, F \rangle$, where $Q$ is a finite set of states, $\Sigma$ is the alphabet, $\Delta :Q\times (\Sigma \cup \{\epsilon \})\rightarrow P(Q)$ is the transition function, $q_0 \in Q$ is the initial state and $F \subseteq Q$ are the terminal states. An NFA can be represented as a directed graph whose adjacency matrix is defined by the transition function, with edge labels representing symbols from the alphabet and binary node labels indicating whether the node is a terminal or nonterminal state.

We pose the problem as a few-shot generative modeling task, where the input is a node embedding consisting of the query text and local graph context, and the output is a regular expression retrieving the link target with high precision.

Let $\mathcal D$ be a document graph, constructed by semantically parsing the document's contents, and neighboring documents from the link graph. Let $\mathcal T$ be a code token, corresponding to a node in the document graph $\mathcal D$.

Our synthesizer is trained on a node embedding from the local graph context, a semantic graph parsed from the parent document and neighboring documents in the link graph. Instead of directly performing link prediction, we train our generative model to output an NFA. We then compare precision and recall over the meta-test set.

\citet{brzozowski1964derivatives} defines the derivative of a language, $\mathcal Q$ with respect to a string \tinline{t} as follows:

\begin{equation}
    \frac{\partial}{\partial\tinline{t}\texttt{\footnotesize t}}\mathcal Q = \{ s | \tinline{t}\texttt{\footnotesize t}s \in \mathcal Q\}
\end{equation}

As noted by Brzozowski, if we interpret the operators ` $|$ ' and ` $\cdot$ ' from \autoref{eq:grammar} as ` $+$ ' and ` $\times$ ' respectively, we recover the standard rules from differential calculus:

% http://joypy.osdn.io/notebooks/Derivatives_of_Regular_Expressions.html#reversing-the-derivatives-to-generate-matching-strings
\begin{equation}
    \frac{\partial\tinline{t}\texttt{\footnotesize t}}{\partial\tinline{t}\texttt{\footnotesize t}} = \epsilon
\end{equation}
\begin{equation}
    \frac{\partial\epsilon}{\partial\tinline{t}\texttt{\footnotesize t}} = \varnothing
\end{equation}
\begin{equation}
    \frac{\partial\mathcal Q*}{\partial\tinline{t}\texttt{\footnotesize t}} = \frac{\partial\mathcal Q}{\partial\tinline{t}\texttt{\footnotesize t}}\mathcal Q*
\end{equation}
\begin{equation}
    \frac{\partial\neg\mathcal Q}{\partial\tinline{t}\texttt{\footnotesize t}} = \neg\frac{\partial\mathcal Q}{\partial\tinline{t}\texttt{\footnotesize t}}
\end{equation}
\begin{equation}
    \frac{\partial\mathcal Q\cdot\mathcal S}{\partial\tinline{t}\texttt{\footnotesize t}} = \frac{\partial\mathcal Q}{\partial\tinline{t}\texttt{\footnotesize t}} \cdot \mathcal S \cup \frac{\partial\mathcal S}{\partial\tinline{t}\texttt{\footnotesize t}} \cdot \mathcal Q
\end{equation}

Given some regex corresponding to a regular language $\mathcal R$\footnote{Hereafter, we use $\mathcal R$ to denote the regex and the language it recognizes interchangeably.}, this tells us how symbolic changes to the regex will change the language it recognizes. Suppose we have a corpus $\mathcal C$ and a set of target documents $\mathcal D^* \subseteq \mathcal C$. We define a loss, $\mathcal L_{\mathcal R}: \langle \mathcal R , \mathcal C , \mathcal D^*\rangle \mapsto \mathbb R$ as follows:

\begin{equation}
    \mathcal L(\mathcal R, \mathcal C, \mathcal D^*) = \mathbb{E}_{\mathcal D^*\sim \mathcal C}\frac{|\overbrace{\mathcal R \cap \mathcal C}^{P}| + |\overbrace{\mathcal C \setminus \mathcal R}^{N}|}{|\underbrace{\mathcal R \cap \mathcal D^*}_{TP}| + |\underbrace{\mathcal C \setminus \mathcal D^* \setminus \mathcal R}_{TN}|}
\end{equation}

Given some set of strings we want to recognize, this ratio tells us how many documents from the corpus $\mathcal C$ does the regex accept $|\mathcal R \cap \mathcal C|$ and reject $|\mathcal C \setminus \mathcal R|$, over how many documents it should accept and reject. We need this ratio to be as low as possible for effective retrieval.

Suppose we have a function $\mathcal G_{\mathbf \theta}: \mathbb{R}^n \times \mathbb{R}^{|\mathbf \theta|} \rightarrow \mathcal R$, which takes a contextual entity embedding $\mathbb R^n$, a set of parameters $\theta$, and returns a regular expression $\mathcal R$. Our goal is to minimize $\mathcal L(\mathcal G_{\mathbf \theta}, \mathbf X, \mathbf Y)$, where $\mathbf X$ is a set of unlabeled context embeddings and $\mathbf Y$ is the ground truth link target. To compute $\nabla_\theta \mathcal L(\mathcal G)$, we need $\nabla_\Sigma \mathcal R$, the vector of partial derivatives of $\mathcal R$ with respect to every symbol in the alphabet, $\Sigma$. \citet{brzozowski1964derivatives} shows us how to backpropogate loss through $\mathcal R$, into parameter space $\mathbb{R}^{|\mathbf \theta|}$.

\begin{figure}
    \centering
    \begin{tikzpicture}[{every node/.style={black,font=\sffamily\Large}}]
    \clip (-3,-3) rectangle (6.5,3);
    \def\ccirc{(0,0) circle (2cm)}
    \def\cccirc{(0,0) circle (3cm)}
    \def\rcirc{(3.5,0) circle (3cm)}
    \def\nullcirc{(0,5.1) circle (4.5cm)}
    \def\boundingbox{(-3,-3) rectangle (6,4.5)}

    \definecolor{handsome}{HTML}{FFFFFF}
    \definecolor{jerk}{HTML}{EF3A43}
    \definecolor{batman}{HTML}{FBC405}
    \definecolor{dumb}{HTML}{B7CA54}
    \definecolor{smart}{HTML}{C7DAC4}
    \definecolor{nerd}{HTML}{FF4444}
    \definecolor{nice}{HTML}{FF8888}

    % fill circles
    \fill[dumb] \cccirc;
    \fill[smart] \ccirc node {\Huge$\mathcal C$};
    \fill[handsome] \nullcirc;
    \fill[nice, opacity=0.5] \rcirc node {\Huge$\mathcal R$};

    % fill intersections

    \begin{scope}
        \clip \boundingbox \rcirc;
        \clip \ccirc;
        \fill[nerd] \nullcirc;
    \end{scope}
    \begin{scope}
        \clip \boundingbox \nullcirc;
        \clip \ccirc;
        \fill[nerd] \rcirc;
    \end{scope}
    \begin{scope}
        \clip \boundingbox \ccirc;
        \clip \cccirc;
        \fill[nice] \rcirc;
    \end{scope}
    \begin{scope}
        \clip \boundingbox \ccirc;
        \clip \cccirc;
        \fill[nice] \nullcirc;
    \end{scope}
    \begin{scope}
        \clip \boundingbox \ccirc;
        \clip \rcirc;
        \clip \cccirc;
        \fill[dumb] \nullcirc node[xshift=1.8cm, yshift=-1cm];
    \end{scope}
    \begin{scope}
        \clip \ccirc;
        \clip \rcirc;
        \clip \nullcirc;
        \fill[green] \boundingbox;
    \end{scope}

    \node at (1.15,1.08) {\Large TP};
    \node at (0.25,2.0) {\Huge $\mathcal D^{*}$};
    \node at (-0.2,1.2) {\Large FN};
    \node at (-0.9,-0.9) {\Large TN};
    \node at (1.3,-0.3) {\Large FP};
\end{tikzpicture}
\end{figure}


\section{Preliminary Results and Discussion}

We compare precision on our benchmark using top-K rank retrieval with respect to various baselines. For instance, to compute the count-search baseline, we search our corpus for the hyperlink anchor text, and rank the resulting documents by frequency of the anchor text. Results are shown in \autoref{fig:baseline_comparison}.

\begin{figure}[H]
\centering
\begin{tikzpicture}
    \begin{axis}[title={Precision at top-K retrieval using count-based ranking}, xlabel=K, ylabel=Precision, width=\textwidth, height=0.4\textwidth, legend pos=north west, ybar]
        \legend{Count search baseline}
        \addplot coordinates {
            (1, 0.395083932853717)
            (2, 0.696043165467626)
            (3, 0.803357314148681)
            (4, 0.84652278177458)
            (5, 0.876498800959233)
            (6, 0.898681055155875)
            (7, 0.908872901678657)
            (8, 0.924460431654676)
            (9, 0.928057553956834)
        };
    \end{axis}
\end{tikzpicture}
\caption{Results for count-based lexical matching baseline.}
\label{fig:baseline_comparison}
\end{figure}

Preliminary results indicate ~40\% of all links in our dataset point to the document in which the anchor text occurs most frequently in the corpus, and ~93\% of all links refer to documents where the ground truth link occurs in the top-10 results ranked by frequency of the anchor text. In future work, we will compare performance against a graph neural network using a word-embedding approach, trained to minimize cosine distance between the source document and target document's context, using a set of candidate links returned by the count-search baseline. Finally, we will compare our approach against the count-search baseline. To do so, we will need to train a synthesizer using our loss function over the regex. Furthermore, we will need to construct the semantic graph embedding over the surrounding context.

\clearpage
\newpage

\bibliography{neurips_2019}
\bibliographystyle{plainnat}
\end{document}
