\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphviz}
\usepackage{auto-pst-pdf}

\title{A Common Graph Representation for\\Source Code and Developer Documentation}

\author{Breandan Considine\\
breandan.considine@mail.mcgill.ca\\
McGill University}

\begin{document}

\maketitle

\begin{abstract}
Semantic information plays a key role in the code search and synthesis settings. In this work, we propose a graph-based representation for source code and natural language which incorporates semantic and relational features from both domains. We apply this graph to a parsing a corpus of code and developer documents, and demonstrate the effectiveness of a common graph-based representation on three downstream tasks: code search, document recommendation and link prediction.
\end{abstract}

\section{Background and motivation}

In addition to its syntactic structure, source code contains a rich denotational and operational semantics~\citep{henkel2018code}. To effectively reason about code in semantically similar but syntactically diverse settings requires models which incorporate features from the call graph~\citep{gu2016deep, 10.1145/3361242.3362774} and surrounding typing context~\citep{allamanis2017learning}. Many semantic features, such as data and control flow~\citep{si2018learning} can be represented as a directed acyclic graph (DAG), which admits linear-time solutions to a number of graph problems, including topological sorting, single-source shortest path and reachability queries.

DAGs also have important applications in natural language parsing~\citep{sagae2008shift, quernheim2012dagger}. Various attempts to build semantic representations for natural language have been proposed, notably the pointer network architecture~\citep{vinyals2015pointer, vinyals2015order}. Pointer networks help to capture permutation-invariant semantic relations between natural language entities, and have important applications in dependency parsing~\citep{ma2018stack}, named-entity recognition~\citep{lample2016neural}, and other tasks where sequence representations fall short. \citet{li2017code} extend pointer networks with a copy-mechanism to handle out-of-vocabulary code tokens.

Content recommendation for doc-to-doc (D2D) and code-to-code (C2C) is a relatively straightforward application of existing link prediction~\citep{zhang2018link} and code embedding~\citep{gu2018deep} techniques, but cross-domain transfer remains largely unsolved. \citet{robillard2015recommending, robillard2017demand} first explore the task of predicting reference API documentation from source code using manual annotation. Prior work also studies the association between comments and code entities~\citep{panthaplackel2020associating} using machine learning, but only within source code.

Maintainers of widely-used software projects often publish web-based documentation, typically stored in markup languages like HTML or Markdown. These files contain a collection of natural language sentences, markup, and hyperlinks to other documents. Both the link graph and the document tree contain important semantic information: the markup describes the text in relation to the other entities in the document hierarchy~\citep{yang2016hierarchical}, while the link graph describes the relationship between the parent document and related documents or source code entities. Documents occasionally contain hyperlinks to source code, but source code rarely contains links to developer documents.

Some programming languages allow users to specify which type of values will inhabit a given variable at runtime. Types allow the compiler to reason about certain properties like nullity~\citep{ekman2007pluggable} and shape~\citep{considine2019kotlingrad}. While types many not appear explicitly in source code, they can often be inferred from the surrounding context using a dataflow graph (DFG). The Java language recently introduced local variable type inference~\citet{liddell2019analyzing}, which allows variable types to be omitted, and later inferred by the compiler.

\section{Proposed approach}

Given a single token in either source code or developer documentation and its surrounding context, what are the most relevant source code or documentation entities related to the token in question? We would like to infer which entities are relevant to a particular token, based on the semantic context. To infer links across these two domains requires building a multi-relational graph, using features extracted from both natural language and source code. Following \citet{si2018learning, gu2018deep, 10.1145/3361242.3362774}, we use a node embedding on the dataflow graph and type environment, and following \citet{yang2016hierarchical, zhang2018link}, use the markup hierarchy and link graph to construct an embedding for code-like tokens used within documentation.

To compensate for the sparsity of hyperlinks between code and documentation, we must design a heuristic to connect the documentation graph and source code entities. One heuristic which developers often use to discover relevant documents is plaintext search on a salient lexical string. Co-occurrence of an infrequent token indicates the two entities are likely related, even though they may not share an explicit grammatical link. If we can recover this relationship without observing the lexical token itself, only using dataflow and type-related information, this indicates our representation is providing useful information.

\section{Data availability and computational requirements}

Java, one of the most popular programming languages on GitHub, is a statically typed language with an extensive amount of API documentation on the web. It has a variety of tools for parsing and analyzing both code~\citep{kovalenko2019pathminer} and natural language~\citep{manning2014stanford, grella2018non}, making it a suitable candidate both as a dataset and implementation language. Our dataset consists of Java repositories on GitHub, and their accompanying docs on the Zeal software documentation aggregator. All projects in our dataset have a collection of source code files and multiple related repositories on GitHub.

We construct two datasets consisting of naturally-occurring links between developer documentation and source code, and a surrogate set of links constructed by matching lexical tokens available in both domains. Our target is the recovery of ground truth links in our test set and surrogate links in the lexical matching graph. By adding weighted edges between source code and documentation and learning the relations using, e.g. a pointer network architecture, we evaluate our approach on reconstructing synthetic links between tokens contained in code-like fragments and markup entities which refer to the selected token. In addition, we evaluate our approach both on D2D and C2C link retrieval, as well as precision and recall on the surrogate link graph.

To perform our experiments, we require a large number of CPUs for semantic parsing, link extraction and graph preprocessing, and a single P100 GPU for training a graph neural network. We have applied and received access to the Niagra CC cluster.

\section{Data}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

Consider the following AST, parsed from a Java project:

\begin{figure}
    \centering
    \input{ast}
    \caption{AST from a simple Java file.}
    \label{fig:ast}
\end{figure}

Consider the following dependency graph, parsed from a Javadoc:

\begin{figure}
    \centering
    \input{eng}
    \caption{Dependency parse graph from a Javadoc.}
    \label{fig:eng}
\end{figure}
\clearpage
\newpage

\bibliography{neurips_2019}
\bibliographystyle{plainnat}
\end{document}