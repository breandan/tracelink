\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphviz}
\usepackage{auto-pst-pdf}
\usepackage{listings}
\usepackage{float}
\usepackage{xcolor}
\usepackage{framed}
% Code listings
%\usepackage[scaled]{beramono}
\usepackage{fontspec}
\setmonofont{JetBrains Mono}
\usepackage{ocr}
\usepackage[skins,breakable,listings]{tcolorbox}

\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{automata, positioning, arrows}

\tikzset{
    node distance=3cm, % specifies the minimum distance between two nodes. Change if necessary.
%    every state/.style={thick, fill=gray!10}, % sets the properties for each ’state’ node
    initial text=$ $, % sets the text that appears on the start arrow
}

\lstdefinelanguage{kotlin}{
    comment=[l]{//},
    commentstyle={\color{gray}\ttfamily},
    emph={delegate, filter, firstOrNull, forEach, it, lazy, mapNotNull, println, repeat, assert, with, head, tail, len, return@},
    numberstyle=\noncopyable,
    emphstyle={\color{OrangeRed}},
    identifierstyle=\color{black},
    keywords={abstract, actual, as, as?, break, by, class, companion, continue, data, do, dynamic, else, enum, expect, false, final, for, fun, get, if, import, in, infix, interface, internal, is, null, object, open, operator, override, package, private, public, return, sealed, set, super, suspend, this, throw, true, try, typealias, val, var, vararg, when, where, while, tailrec, reified},
    keywordstyle={\color{NavyBlue}\bfseries},
    morecomment=[s]{/*}{*/},
    morestring=[b]",
    morestring=[s]{"""*}{*"""},
    ndkeywords={@Deprecated, @JvmField, @JvmName, @JvmOverloads, @JvmStatic, @JvmSynthetic, Array, Byte, Double, Float, Boolean, Int, Integer, Iterable, Long, Runnable, Short, String},
    ndkeywordstyle={\color{BurntOrange}\bfseries},
    sensitive=true,
    stringstyle={\color{ForestGreen}\ttfamily},
    literate={`}{{\char0}}1
}

\usepackage{lipsum}
\tcbuselibrary{listings,breakable}

\newtcblisting{javalisting}[1][]{%
    breakable=false,
    listing only,
    boxsep=-1.5pt,
    top=-1pt,
    bottom=-0.5pt,
    listing options={
        language=kotlin,
        basicstyle=\ttfamily\small,
        numberstyle=\footnotesize\noncopyable,
        tabsize=2,
        numbers=right,
        breaklines=true,
        inputencoding=utf8,
        escapeinside={(*@}{@*)},
        #1
    },
}

\tcbset{
    enhanced jigsaw,
    breakable,
    listing only,
    boxsep=-1pt,
    top=-1pt,
    bottom=-0.5pt,
    right=-0.5pt,
    overlay first={
        \node[black!50] (S) at (frame.south) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.south west) -- (S) -- (frame.south east);
    },
    overlay middle={
        \node[black!50] (S) at (frame.south) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.south west) -- (S) -- (frame.south east);
        \node[black!50] (S) at (frame.north) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.north west) -- (S) -- (frame.north east);
    },
    overlay last={
        \node[black!50] (S) at (frame.north) {\Large\ding{34}};
        \draw[dashed,black!50] (frame.north west) -- (S) -- (frame.north east);
    },
    before={\par\vspace{10pt}},
    after={\par\vspace{\parskip}\noindent}
}

\newcommand*{\inlineimg}[1]{%
    \raisebox{-.3\baselineskip}{%
        \includegraphics[
            height=\baselineskip,
            width=\baselineskip,
            keepaspectratio,
        ]{#1}%
    }%
}

\definecolor{slightgray}{rgb}{0.90, 0.90, 0.90}

\usepackage{soul}
\makeatletter
\def\SOUL@hlpreamble{%
    \setul{}{3.0ex}%
    \let\SOUL@stcolor\SOUL@hlcolor%
    \SOUL@stpreamble%
}
\makeatother

\newcommand{\tinline}[1]{%
    \begingroup%
    \sethlcolor{slightgray}%
    \hl{\ttfamily\footnotesize #1}%
    \endgroup
}

\title{Learning to parse developer documentation}

\author{Breandan Considine\\
breandan.considine@mail.mcgill.ca\\
McGill University}

\begin{document}

\maketitle

\begin{abstract}
Semantic information plays a key role in the code search and synthesis settings. In this work, we propose a regular expression synthesizer for parsing source code and API documentation which incorporates semantic and relational information the surrounding context. We apply our synthesizer to a link prediction task on a corpus of Java code and developer docs, and demonstrate the effectiveness of model-based representation learning for link prediction in the source-to-source and doc-to-doc setting.
\end{abstract}

\section{Introduction}

In addition to its syntax, source code contains a rich denotational and operational semantics~\citep{henkel2018code}. To effectively reason about code in semantically similar but syntactically diverse settings requires models which incorporate features from the call graph~\citep{gu2016deep, liu2019neural} and surrounding typing context~\citep{allamanis2017learning}. Many semantic features, such as data and control flow~\citep{si2018learning} can be represented by a directed acyclic graph (DAG), which admits linear-time solutions to many graph problems, including topological sorting, single-source shortest path and reachability queries.

The field of natural language has also developed a rich set of graph-based representations, including \citet{reddy2016transforming}'s and other typed attribute grammars which can be used to reason about syntactic and semantic relations between natural language entities. The pointer network architecture~\citep{vinyals2015pointer, vinyals2015order} can be used to construct permutation-invariant semantic relations between entities, and has important applications in dependency parsing~\citep{ma2018stack}, named-entity recognition~\citep{lample2016neural}, and other semantic parsing tasks where sequence-based representations fall short. \citet{li2017code} extend pointer networks with a copy-mechanism to handle out-of-vocabulary code tokens.

Prior work has studied dataflow and datatypes in code~\citep{si2018learning, gu2018deep, liu2019neural}, as well as document hierarchy~\citep{yang2016hierarchical} and link-based~\citep{zhang2018link} learning representations. Entity linking in doc-to-doc (D2D) and source-to-source (S2S) is a straightforward application of link prediction~\citep{zhang2018link} and code embedding~\citep{gu2018deep} techniques, but cross-domain transfer remains challenging. \citet{robillard2015recommending, robillard2017demand} first explore the task of predicting reference API docs from source code using manual annotation. Prior work also studies the association between comments and code entities~\citep{iyer2018mapping, panthaplackel2020associating} using machine learning, but only within source code.

Maintainers of widely-used software projects often publish web-based developer docs, typically stored in markup languages like HTML or Markdown~\citep{terrasa2018using}. These files contain a collection of natural language sentences, markup, and hyperlinks to other documents and source code entities. Both the document tree and link graph contain important semantic information: the markup describes the text in relation to the other entities in the document hierarchy~\citep{yang2016hierarchical}, while the link graph describes the relationship between the parent document and related documents or source code entities. Documents occasionally contain hyperlinks to source code, but source code rarely contains links to developer documents. To compensate for the sparsity of hyperlinks between code and documentation, new techniques are required.

Unlike natural language where polysemy is common~\citep{ganea2016probabilistic}, named entities in most programming languages are relatively unique, even across unrelated APIs. Given a single token in source code or documentation, a skilled developer can quickly locate the referent, even without prior familiarity with the API in question, by using some contextual cues. For example, we observe the string \tinline{AbstractSingletonProxyFactoryBean} has the following properties:

\begin{enumerate}
    \item The string is camel-case, indicating it refers to an entity in a camel-case language.
    \item The string contains the substring \tinline{Bean}, a common token in the Java language.
    \item The string begins with a capital letter, indicating it refers to a class or interface.
\end{enumerate}

Developers often use a tool called \tinline{grep} to locate files, which accepts queries written in the regular expression (regex)language, a domain specific language for string parsing. Skilled \tinline{grep} users are able to rapidly construct a regular expression which retrieves the document with high probability whilst omitting irrelevant results. Assuming the aforementioned entity exists on a filesystem, one might simply execute the following command to locate it:

\centerline{\tinline{\$ grep -r --include .java "class AbstractSingletonProxyFactoryBean" .}}

We hypothesize it is possible to construct a short query which uniquely identifies any named entity (assuming it exists) in a corpus of software documents and furthermore, it is possible to learn a program which synthesizes a query retrieving the canonical entity with high probability, given a named entity reference and its surrounding documentation context.

\section{Background}

\newcommand{\mor}{\ensuremath{\;\mid\;}}
\newcommand{\code}[1]{\ensuremath{\texttt{\tinline{#1}}}}
\newcommand{\bnfrl}[1]{\ensuremath{\langle#1\rangle}}

Let $\Sigma = \{$\tinline{A}, \tinline{a}, \ldots, \tinline{Z}, \tinline{z}, \tinline{0}, \tinline{1}, \ldots, \tinline{9}, \tinline{\$}, \tinline{\textasciicircum}$\}$. Our language $J_{<}$ has the following productions:

\begin{equation}
    \bnfrl{exp} ::= \bnfrl{exp}\bnfrl{exp} \mor \bnfrl{exp}|\bnfrl{exp} \mor \code{(}\bnfrl{exp}\code{)} \mor \alpha \in \Sigma \mor \bnfrl{exp}\code{*} \mor
    \code{.}
\end{equation}

An expression in $J_<$ is a regular expression, reducible to a non-deterministic finite automaton (NFA) using Glushkov's algorithm ~\citep{glushkov1961abstract}, independently discovered by McNaughton-Yamada-Thompson, as shown in \autoref{fig:regex_to_nfa}. NFA are reducible to both deterministic finite automata (DFA) using the powerset construction~\citep{rabin1959finite} and regular expressions using Arden's Lemma~\citep{arden1961delayed}. It is also possible to convert regular expressions directly to DFA using the Berry-Sethi Algorithm~\citep{berry1986regular}.

% https://www3.nd.edu/~kogge/courses/cse30151-fa17/Public/other/tikz_tutorial.pdf
% Glushkov's algorithm: https://www.irif.fr/~jep/PDF/MPRI/MPRI.pdf#subsection.3.5.2
\begin{figure}
    \begin{tikzpicture}[->, >=stealth,]
        \node[state, initial, accepting] (q1) {$q_1$};
        \node[state, left of=q1] (q2) {$q_2$};
        \node[state, accepting, left of=q2] (q3) {$q_3$};
        \node[state, accepting, right of=q1] (q4) {$q_4$};
        \node[state, above right of=q4] (q5) {$q_5$};
        \node[state, accepting, below right of=q4] (q6) {$q_6$};
        \draw
%        (q1) edge[loop above] (q1)
        (q1) edge node{\tinline a} (q2)
        (q1) edge node{\tinline b} (q4)
        (q2) edge[bend right] node{\tinline a} (q3)
        (q3) edge[bend right] node{\tinline b} (q2)
        (q4) edge[loop above] node{\tinline a} (q4)
        (q4) edge node{\tinline a} (q5)
        (q6) edge node{\tinline a} (q4)
        (q5) edge[bend left] node{\tinline b} (q6)
        (q6) edge[bend left] node{\tinline a} (q5)
        ;
%        (q2) edge[bend left, above] node{0} (q3)
        ;
%        (q2) edge[bend left, above] node{0} (q3)
%        (q3) edge[bend left, below] node{0, 1} (q2);
    \end{tikzpicture}
    \caption{NFA corresponding to the regular expression \tinline{(a(ab)*)*(ba)*}.}
    \label{fig:regex_to_nfa}
\end{figure}

Formally, an NFA is a 5-tuple $\langle Q, \Sigma, \Delta, q_0, F \rangle$, where $Q$ is a finite set of states, $\Sigma$ is the alphabet, $\Delta :Q\times (\Sigma \cup \{\epsilon \})\rightarrow P(Q)$ is the transition function, $q_0 \in Q$ is the initial state and $F \subseteq Q$ are the terminal states. An NFA can be represented as a directed graph whose adjacency matrix is defined by the transition function, with edge labels representing symbols from the alphabet and binary node labels indicating whether the node is a terminal or nonterminal state.

We pose the problem as a few-shot generative modeling task, where the input is a local graph consisting of the query text and graph context, and the output is an adjacency matrix defining the NFA. Our goal is to synthesize an NFA which accepts the target document and no other documents or as few others as possible from the entire corpus.

\section{Method}

Let $\mathcal D$ be a document graph, constructed by semantically parsing the document's contents, and neighboring documents from the link graph. Let $\mathcal T$ be a code token, corresponding to a node in a document graph $\mathcal D$. Following prior work in few-shot meta-learning on graphs, we adapt the training procedure suggested by \citet{bose2019meta} to pretrain a meta learner $\mathcal M$ on the following objective:

\begin{equation}
    \mathcal L_{G} = \mathbb E_{q_\phi}[\log p(A^{train}|Z)] - KL[q_\phi(Z|X, A^{train}) || p(z)]
\end{equation}

Our synthesizer is trained on a local context, a semantic graph parsed from the parent document and neighboring documents from the link graph. Instead of directly performing link prediction, we train a generative model to output an NFA, which can be directly translated to a regular expression to select the candidate documents. Using the pretrained embedding, we then rank each candidate document using the Weisfeiler-Lehman similarity~\citep{shervashidze2011weisfeiler}, then select a single document to link. We then compare precision and recall over the meta-test set.

\section{Dataset}

Java, one of the most prolific programming languages on GitHub, is a statically typed language with a high volume of API documentation. Offering a variety of tools for source code~\citep{parr2013definitive, hosseini2013javaparser, kovalenko2019pathminer} and natural language~\citep{manning2014stanford, grella2018non} parsing, it is a convenient language for both analysis and implementation. Our dataset consists of Java repositories on GitHub, and their accompanying developer documents. All projects in our dataset have a collection of source code files and multiple related repositories on GitHub.

We construct two datasets consisting of naturally-occurring links between developer docs and source code, and a surrogate set of links constructed by matching lexical tokens available in both domains. Our target is recovery of ground truth links in the test set and surrogate links in the lexical matching graph. We first add weighted edges between code and docs, then evaluate our approach by predicting synthetic links between tokens contained in code fragments and markup entities which refer to the selected token. In addition, we evaluate our approach on both D2D and C2C link retrieval, as well as precision and recall on the surrogate link relations.

Our data consists of two complementary datasets: abstract syntax trees collected from Java source code and developer documentation. We use the astminer~\citep{kovalenko2019pathminer} library to parse Java code, jsoup~\citep{hedley2009jsoup} to parse HTML and Stanford's CoreNLP~\citep{manning2014stanford} library to parse dependency graphs from developer docs. Consider the following AST, parsed from the \href{https://www.eclipse.org/collections/}{Eclipse Collections} Java project:

\vspace{20pt}

\begin{figure}[H]
    \centering
    \input{ast}
%    \caption{This AST was parsed from the following code snippet:}
    \label{fig:ast}
\end{figure}

\vspace{-30pt}The AST depicted above was generated by parsing the following code snippet:

\begin{javalisting}
public void lastKey_throws() {
    new ImmutableTreeMap<>(new TreeSortedMap<>()).lastKey();
}
\end{javalisting}

Now consider the following dependency graph, taken from a Javadoc in the same project:

\begin{figure}[H]
    \centering
    \input{eng}
    \caption{This graph was parsed from the following comment: ``The advantages to using this container over a \footnotesize\tinline{Map<K, Collection<V>{}>} is that all of the handling of the value collection can be done automatically. It also allows implementations to further specialize in how duplicate values will be handled. Value collections with list semantics would allow duplicate values for a key, while those implementing set semantics would not. The value collections can never be empty.''}
    \label{fig:eng}
\end{figure}

Our goal is to connect these two graphs using a common model for source code and natural language. Absent any explicit \tinline{@link} or \tinline{@see} annotations, in order to relate these two graphs, we must somehow infer the shared semantic entities, which we can do for a subset using a simple lexical matching procedure. Inferring semantically relevant links however requires a more meaningful representation.

\section{Preliminary Results}

We compare precision on our benchmark using top-K rank retrieval with respect to various baselines. For instance, to compute the count-search baseline, we search our corpus for the hyperlink anchor text, and rank the resulting documents by the frequency of the anchor text. Results are shown in \autoref{fig:baseline_comparison}.

\begin{figure}[H]
\centering
\begin{tikzpicture}
    \begin{axis}[title={Precision at top-K retrieval using count-based ranking}, xlabel=K, ylabel=Precision, width=\textwidth, height=0.4\textwidth, legend pos=north west, ybar]
        \legend{Count search baseline}
        \addplot coordinates {
            (1, 0.395083932853717)
            (2, 0.696043165467626)
            (3, 0.803357314148681)
            (4, 0.84652278177458)
            (5, 0.876498800959233)
            (6, 0.898681055155875)
            (7, 0.908872901678657)
            (8, 0.924460431654676)
            (9, 0.928057553956834)
        };
    \end{axis}
\end{tikzpicture}
\caption{Preliminary results.}
\label{fig:baseline_comparison}
\end{figure}

\section{Discussion}

Preliminary results indicate ~40\% of all links in our dataset point to the document in which the anchor text occurs most frequently across the corpus, and ~93\% of all links refer to documents where the ground truth link occurs in the top-10 results ranked by frequency of the anchor text. In future work, we will compare performance against a neural network using a simple word-embedding approach, trained to minimize cosine distance between the source document and target document's context, using a set of candidate links returned by the count-search baseline. Finally, we will compare our approach against the count-search baseline.

In the weeks, we will need to train a synthesizer using a yet-to-be-determined loss function over the regular expression. Prior literature suggests there may be a way to backpropogate through the regular expression synthesizer using the ~\citet{brzozowski1964derivatives} derivative. Furthermore, we will need to construct the semantic graph embedding over the surrounding context. This will require some additional consideration.

\clearpage
\newpage

\bibliography{neurips_2019}
\bibliographystyle{plainnat}
\end{document}
